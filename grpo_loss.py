import torch
from torch.nn import functional as F


def compute_advantages(
    rewards: torch.Tensor  # [B, G]
) -> torch.Tensor:
    mean = rewards.mean(dim=1, keepdims=True)
    std = rewards.std(dim=1, keepdims=True)
    return (rewards - mean) / (std + 1e-8)


def grpo_loss(
    logits: torch.Tensor,  # [B, T, V]
    labels: torch.Tensor,  # [B, T]
    output_masks: torch.Tensor,  # [B, T]
    old_logps: torch.Tensor,  # [B, T]
    ref_logps: torch.Tensor,  # [B, T]
    advantages: torch.Tensor, # [B]
    eps: float = 0.2,
    beta: float = 0.01,
) -> torch.Tensor:
    # Compute logprobs from policy logits.
    logps = F.log_softmax(logits.float(), dim=-1)
    logps = torch.gather(logps, dim=-1, index=labels.unsqueeze(-1)).squeeze()
    # Sanity check we get the same value using F.cross_entropy.
    ce_logps = -F.cross_entropy(logits.float().view(-1, logits.shape[-1]), labels.view(-1), reduction='none')
    torch.testing.assert_close(logps.view(-1), ce_logps)

    # Compute lower bound surrogate objective.
    ratios = torch.exp(logps - old_logps)
    clipped_ratios = torch.clamp(ratios, 1 - eps, 1 + eps)
    lower_bound = torch.minimum(ratios * advantages, clipped_ratios * advantages)

    # Estimate D_{KL}[pi || pi_ref] using K3 estimator.
    k3 = torch.exp(ref_logps - logps) - (ref_logps - logps) - 1
    # NOTE: The k3 estimate above was what was originally implemented in the DeepSeek Math, DeepSeek-V3 and 
    # DeepSeek R1 papers. However it is an incorrect (biased) estimate of the KL divergence because the samples
    # have been generated by pi_old and not pi. Like in the DeepSeek-V3.2 paper, we correct the bias using
    # the ratio below.
    unbiased_k3 = torch.exp(logps - old_logps) * k3

    loss = ((lower_bound - unbiased_k3)  * output_masks).sum() / output_masks.sum(dim=1)
    return loss.mean()


if __name__ == "__main__":
    torch.manual_seed(42)

    B, G, T, V = 4, 8, 128, 64
    logits = torch.randn(size=(B*G, T, V))
    labels = torch.randint(0, V, size=(B*G, T))
    output_masks = torch.ones(size=(B*G, T))
    for i in range(B):
        output_masks[i, :torch.randint(0, T//2, size=(1,)).item()] = 0
    output_masks = output_masks.bool()
    old_logps = torch.log(torch.rand(size=[B*G, T]))
    ref_logps = torch.log(torch.rand(size=(B*G, T)))

    rewards = torch.randint(0, 2, size=(B, G)).float()
    advantages = compute_advantages(rewards).view(B*G, 1)

    loss = grpo_loss(
        logits=logits, 
        labels=labels, 
        output_masks=output_masks,
        old_logps=old_logps, 
        ref_logps=ref_logps,
        advantages=advantages,
    )
    print(loss)
    